{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from InSituToolkit.imaging_database import write_experiment\n",
    "from helpers import getPositions, getIDs\n",
    "import imaging_db.database.db_operations as db_ops\n",
    "import imaging_db.filestorage.s3_storage as s3_storage\n",
    "import imaging_db.filestorage.local_storage as local_storage\n",
    "import imaging_db.utils.db_utils as db_utils\n",
    "import os, csv, pickle\n",
    "\n",
    "# information taken from InSituToolkit notebooks, which happened to use the LungInsitu dataset\n",
    "# we must pull all image id's that contain \"SDG\" and make an experiment for them.\n",
    "# We assume that each file is a single round of imaging\n",
    "\n",
    "spot_channels = ['Opal_570', 'Opal_620', 'Opal_690']\n",
    "nuc_channel = ['DAPI']\n",
    "db_credentials = '/Users/andrew.cote/Documents/db_credentials.json'\n",
    "credentials_str = db_utils.get_connection_str(db_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "We want to save the experiment files in a specific format / directory naming convention, i.e.\n",
    "\n",
    "in \"/Users/andrew.cote/Documents/In-Situ_Transcriptomics/LungInSitu/experiments/\" \n",
    "\n",
    "The list of experiments reads like\n",
    "\n",
    "/TH134_E2_B1_assay2/roi1/<experiment_files>\n",
    "/TH134_E2_B1_assay2/roi2/<experiment_files>\n",
    "\n",
    "For ease of analysis we construct a dictionary to be saved with the format:\n",
    "\n",
    "[dataset_id1, dataset_id2, ...]\n",
    "{dataset_id: [savepath, assay#]}\n",
    "\n",
    "'''\n",
    "\n",
    "base_path = \"/Users/andrew.cote/Documents/In-Situ_Transcriptomics/LungInSitu/experiments/\"\n",
    "dict_save_name = 'dict_of_experiments.obj'\n",
    "list_save_name = 'list_of_experiments.obj'\n",
    "dict_of_datasets = {}\n",
    "list_of_datasets = []\n",
    "\n",
    "csv_file = open('../Documentation/metadata_lung.csv')\n",
    "csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "line_count = 0\n",
    "\n",
    "for row in csv_reader:\n",
    "    if line_count > 0:\n",
    "        \n",
    "        # extract relevant columns from the csv file\n",
    "        dataset_id = row[0]\n",
    "        roi = row[-1]\n",
    "        full_name = row[2]\n",
    "        \n",
    "        # construct the directory name according to the convention specified:\n",
    "        idx = full_name.find('assay')\n",
    "        dir_name = full_name[0:idx+6]\n",
    "        assayNo = full_name[idx+5]\n",
    "        save_path = base_path + dir_name\n",
    "        save_path_exp = save_path + '/roi' + roi + '/'\n",
    "        file_name = save_path_exp[76:].replace('/', '-')\n",
    "        \n",
    "        if not os.path.exists(save_path):\n",
    "            os.mkdir(save_path)\n",
    "        \n",
    "        if not os.path.exists(save_path_exp):\n",
    "            os.mkdir(save_path_exp)\n",
    "        \n",
    "            # pull positions from database and write the experiment file\n",
    "            positions = getPositions(db_credentials, dataset_id)\n",
    "        \n",
    "            write_experiment(db_credentials, save_path_exp, [dataset_id],\n",
    "                     spot_channels=spot_channels, nuc_channels=nuc_channel,\n",
    "                     positions=positions\n",
    "                    )\n",
    "            print(\"wr. exp @: \" + save_path_exp)\n",
    "        \n",
    "        dict_of_datasets[dataset_id] = [save_path_exp, file_name, assayNo]\n",
    "        list_of_datasets.append(dataset_id)\n",
    "        \n",
    "    line_count += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open(dict_save_name, 'wb')\n",
    "pickle.dump(dict_of_datasets, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open(list_save_name, 'wb')\n",
    "pickle.dump(list_of_datasets, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are three unique codebooks to be used, depending on which assay number\n",
    "\n",
    "from starfish.types import Axes, Coordinates, Features\n",
    "import pickle\n",
    "\n",
    "codebook_1 = [\n",
    "      {\n",
    "          Features.CODEWORD: [\n",
    "              {Axes.ROUND.value: 0, Axes.CH.value: 0, Features.CODE_VALUE: 1},\n",
    "          ],\n",
    "          Features.TARGET: \"NFIX\"\n",
    "      },\n",
    "      {\n",
    "          Features.CODEWORD: [\n",
    "              {Axes.ROUND.value: 0, Axes.CH.value: 1, Features.CODE_VALUE: 1},\n",
    "          ],\n",
    "          Features.TARGET: \"SFTPD\"\n",
    "      },\n",
    "      {\n",
    "          Features.CODEWORD: [\n",
    "              {Axes.ROUND.value: 0, Axes.CH.value: 2, Features.CODE_VALUE: 1},\n",
    "          ],\n",
    "          Features.TARGET: \"DLC1\"\n",
    "      },\n",
    "  ]\n",
    "\n",
    "codebook_2 = [\n",
    "      {\n",
    "          Features.CODEWORD: [\n",
    "              {Axes.ROUND.value: 0, Axes.CH.value: 0, Features.CODE_VALUE: 1},\n",
    "          ],\n",
    "          Features.TARGET: \"IGFBP3\"\n",
    "      },\n",
    "      {\n",
    "          Features.CODEWORD: [\n",
    "              {Axes.ROUND.value: 0, Axes.CH.value: 1, Features.CODE_VALUE: 1},\n",
    "          ],\n",
    "          Features.TARGET: \"NKX2-1\"\n",
    "      },\n",
    "      {\n",
    "          Features.CODEWORD: [\n",
    "              {Axes.ROUND.value: 0, Axes.CH.value: 2, Features.CODE_VALUE: 1},\n",
    "          ],\n",
    "          Features.TARGET: \"SELENBP1\"\n",
    "      },\n",
    "  ]\n",
    "\n",
    "codebook_3 = [\n",
    "      {\n",
    "          Features.CODEWORD: [\n",
    "              {Axes.ROUND.value: 0, Axes.CH.value: 0, Features.CODE_VALUE: 1},\n",
    "          ],\n",
    "          Features.TARGET: \"SERPINE1\"\n",
    "      },\n",
    "      {\n",
    "          Features.CODEWORD: [\n",
    "              {Axes.ROUND.value: 0, Axes.CH.value: 1, Features.CODE_VALUE: 1},\n",
    "          ],\n",
    "          Features.TARGET: \"GJB3\"\n",
    "      },\n",
    "      {\n",
    "          Features.CODEWORD: [\n",
    "              {Axes.ROUND.value: 0, Axes.CH.value: 2, Features.CODE_VALUE: 1},\n",
    "          ],\n",
    "          Features.TARGET: \"GJB2\"\n",
    "      },\n",
    "  ]\n",
    "\n",
    "codebook = {1:codebook_1, 2:codebook_2, 3:codebook_3}\n",
    "\n",
    "filehandler = open('codebook.obj', 'wb')\n",
    "pickle.dump(codebook, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'codeword': [{'r': 0, 'c': 0, 'v': 1}], 'target': 'IGFBP3'},\n",
       " {'codeword': [{'r': 0, 'c': 1, 'v': 1}], 'target': 'NKX2-1'},\n",
       " {'codeword': [{'r': 0, 'c': 2, 'v': 1}], 'target': 'SELENBP1'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codebook[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
