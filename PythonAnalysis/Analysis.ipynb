{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan of attack:\n",
    "- create a pipeline in Ilastik that can classify regions as stroma or non-stroma\n",
    "- Produce a binary mask where non-stroma regions are '1' and stroma regions are '0'\n",
    "- Logical AND this mask with all three channels on the fovs such that we are only left with non-zero intensities\n",
    "    in non-stroma regions\n",
    "- find spots using bandpass filter and blob-detector\n",
    "- construct codebook according to the type of assay used (will need to read off file-names etc)\n",
    "- produce the count x cell matrix, where there is only 'one' cell which is non-stroma region\n",
    "- estimate total area of non-stroma region in pixels (could just sum elements of binary mask matrix)\n",
    "- produce density of target genes, i.e. number of spots/pixel area, for each channel\n",
    "\n",
    "Some notes:\n",
    "- binary mask will be the same across all channels\n",
    "- Check in with Ashley about whether the stroma and non-stroma classification is valid\n",
    "- could either use the binary mask to force pixel intensities to be zero, or just use it as a label image for the \n",
    "    codebook gene assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from InSituToolkit.imaging_database import write_experiment\n",
    "import imaging_db.database.db_operations as db_ops\n",
    "import imaging_db.filestorage.s3_storage as s3_storage\n",
    "import imaging_db.filestorage.local_storage as local_storage\n",
    "import imaging_db.utils.db_utils as db_utils\n",
    "import os, csv, pickle\n",
    "\n",
    "%gui qt5\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from starfish import Experiment, display, Codebook, ExpressionMatrix, FieldOfView, BinaryMaskCollection, LabelImage, ImageStack\n",
    "from starfish.image import Filter\n",
    "from starfish.spots import FindSpots, DecodeSpots, AssignTargets\n",
    "from starfish.types import Axes, Coordinates, Features, FunctionSource, TraceBuildingStrategies\n",
    "\n",
    "from InSituToolkit.analysis import save_stack\n",
    "\n",
    "db_credentials = '/Users/andrew.cote/Documents/db_credentials.json'\n",
    "root_path = '/Users/andrew.cote/Documents/In-Situ_Transcriptomics/LungInSitu/'\n",
    "\n",
    "def produceMaskFromTif(path: str, img_stack: ImageStack):\n",
    "    \"\"\"\n",
    "    Converts a segmentation TIF file into a binary mask which can be used for assigning spots to cells\n",
    "    \"\"\"\n",
    "    label_image = io.imread(path)\n",
    "\n",
    "    # we must also get the physical coords from the original to produce the binary mask\n",
    "    yc = img_stack.xarray.yc.values\n",
    "    xc = img_stack.xarray.xc.values\n",
    "    physical_ticks = {Coordinates.Y: yc, Coordinates.X:xc}\n",
    "\n",
    "    y = img_stack.xarray.y.values\n",
    "    x = img_stack.xarray.x.values\n",
    "    pixel_coords = {Axes.Y: y, Axes.X: x}\n",
    "\n",
    "    label_im = LabelImage.from_label_array_and_ticks(label_image, \\\n",
    "                                                pixel_ticks=pixel_coords, \\\n",
    "                                                physical_ticks=physical_ticks, \\\n",
    "                                                log = img_stack.log)\n",
    "\n",
    "    mask = BinaryMaskCollection.from_label_image(label_im)\n",
    "    return mask, label_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_datasets = pickle.load(open('list_of_experiments.obj', 'rb'))\n",
    "dict_of_datasets = pickle.load(open('dict_of_experiments.obj', 'rb'))\n",
    "CODEBOOK = pickle.load(open('codebook.obj', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" ALREADY RAN ONCE\\nfor dataset in list_of_datasets:\\n    save_path, exp_name, assayNo = dict_of_datasets[dataset]\\n\\n    exp = Experiment.from_json(save_path + 'experiment.json')\\n    exp_name_safe = exp_name.replace('/', '-')\\n\\n    # get all fovs from the experiment\\n    fovs = [k for k in exp.keys()]\\n\\n    # To import images into Ilastik, we want to take the max projection of z-Planes across all color channels, \\n    # since the stroma tissue flouresces brightly and we want to paint these regions in Ilastik. \\n    for fov in fovs:\\n        img_stack = next(exp[fov].get_images(FieldOfView.PRIMARY_IMAGES))\\n        img_stack_sel = img_stack.sel({Axes.CH: 0})\\n        img_stack_reduced = img_stack_sel.reduce({Axes.ZPLANE}, func='max')\\n        save_stack(img_stack_reduced, root_path + 'ilastik/reduced_fovs/' + exp_name_safe + fov + '.tif')\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" ALREADY RAN ONCE\n",
    "for dataset in list_of_datasets:\n",
    "    save_path, exp_name, assayNo = dict_of_datasets[dataset]\n",
    "\n",
    "    exp = Experiment.from_json(save_path + 'experiment.json')\n",
    "    exp_name_safe = exp_name.replace('/', '-')\n",
    "\n",
    "    # get all fovs from the experiment\n",
    "    fovs = [k for k in exp.keys()]\n",
    "\n",
    "    # To import images into Ilastik, we want to take the max projection of z-Planes across all color channels, \n",
    "    # since the stroma tissue flouresces brightly and we want to paint these regions in Ilastik. \n",
    "    for fov in fovs:\n",
    "        img_stack = next(exp[fov].get_images(FieldOfView.PRIMARY_IMAGES))\n",
    "        img_stack_sel = img_stack.sel({Axes.CH: 0})\n",
    "        img_stack_reduced = img_stack_sel.reduce({Axes.ZPLANE}, func='max')\n",
    "        save_stack(img_stack_reduced, root_path + 'ilastik/reduced_fovs/' + exp_name_safe + fov + '.tif')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:06<00:00,  3.16it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 37.82it/s]\n",
      "100%|██████████| 21/21 [00:06<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fov_000 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:06<00:00,  3.37it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 46.37it/s]\n",
      "100%|██████████| 21/21 [00:09<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fov_001 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:06<00:00,  3.06it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 42.03it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fov_002 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:06<00:00,  3.08it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 42.64it/s]\n",
      "100%|██████████| 21/21 [00:06<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fov_003 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:07<00:00,  2.98it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 42.82it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fov_004 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:07<00:00,  2.83it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 40.96it/s]\n",
      "100%|██████████| 21/21 [00:06<00:00,  3.09it/s]\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fov_005 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:07<00:00,  2.79it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 44.11it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.92it/s]\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fov_006 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:07<00:00,  2.98it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 43.91it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.78it/s]\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fov_007 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:07<00:00,  2.80it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 24.60it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fov_008 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:09<00:00,  2.17it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 43.06it/s]\n",
      "100%|██████████| 21/21 [00:06<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fov_009 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:07<00:00,  2.88it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 42.26it/s]\n",
      "100%|██████████| 21/21 [00:09<00:00,  2.13it/s]\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fov_010 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:07<00:00,  2.77it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 40.54it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fov_011 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "At this point we have exported all the reduced / projected images and then classified them using Ilastik\n",
    "'''\n",
    "SpotFinder = FindSpots.BlobDetector(min_sigma=1,\n",
    "                                    max_sigma=10,\n",
    "                                    num_sigma=10,\n",
    "                                    threshold=0.01,\n",
    "                                    measurement_type='mean'\n",
    "                                   )\n",
    "\n",
    "glp = Filter.GaussianLowPass(sigma=1)\n",
    "ghp = Filter.GaussianHighPass(sigma=3)\n",
    "\n",
    "# TODO: iterate over all datasets: \n",
    "dataset = list_of_datasets[0]\n",
    "\n",
    "save_path, exp_name, assayNo = dict_of_datasets[dataset]\n",
    "codebook = Codebook.from_code_array(CODEBOOK[int(assayNo)])\n",
    "exp = Experiment.from_json(save_path + 'experiment.json')\n",
    "\n",
    "gene_counts_across_fovs = []\n",
    "\n",
    "for fov in exp.keys():\n",
    "    # 1 - already completed\n",
    "    # 2: Filter images and project Zplanes\n",
    "    img_stack = next(exp[fov].get_images(FieldOfView.PRIMARY_IMAGES))\n",
    "    img_stack_f1 = ghp.run(img_stack,  in_place=False)\n",
    "    img_stack_f2 = glp.run(img_stack_f1,  in_place=False)\n",
    "    img_proj_z = img_stack_f2.reduce({Axes.ZPLANE}, func='max')\n",
    "\n",
    "    # 2: Find Spots\n",
    "    spots = SpotFinder.run(img_proj_z)\n",
    "    decoder = DecodeSpots.SimpleLookupDecoder(codebook=codebook)\n",
    "    decoded_intensities = decoder.run(spots=spots)\n",
    "\n",
    "    # 3:\n",
    "    path = root_path + 'ilastik/classified_fovs/' + exp_name + fov + '.tif'\n",
    "    mask, label_image = produceMaskFromTif(path, img_stack)\n",
    "\n",
    "    # 4:\n",
    "    al = AssignTargets.Label()\n",
    "    labeled = al.run(mask, decoded_intensities)\n",
    "    cg = labeled.to_expression_matrix()\n",
    "\n",
    "    # 5:\n",
    "    STROMA_ID = 2\n",
    "    NON_STROMA_ID = 1\n",
    "\n",
    "    xdiff = np.mean(np.diff(img_stack.xarray.xc.values))\n",
    "    ydiff = np.mean(np.diff(img_stack.xarray.yc.values))\n",
    "\n",
    "    area = xdiff * ydiff\n",
    "\n",
    "    pixel_area = np.sum(label_image == NON_STROMA_ID)\n",
    "    physical_area = pixel_area * area\n",
    "\n",
    "    # NOTE: highly likely that 0 == cell_id for non-stroma tissue, yet to be determined however\n",
    "\n",
    "    gene_counts = {}\n",
    "    gene_counts['area'] = physical_area\n",
    "\n",
    "    for target in codebook.indexes['target']:\n",
    "        gene_counts[target] = cg.loc[0, target].values\n",
    "\n",
    "    gene_counts_across_fovs.append(gene_counts)\n",
    "    print(fov + ' completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_area = 0\n",
    "gene_density = {'IGFBP3': 0}\n",
    "\n",
    "for item in gene_counts_across_fovs:\n",
    "    total_area += item['area']\n",
    "    gene_density['IGFBP3'] = gene_density['IGFBP3'] + item['IGFBP3']\n",
    "    \n",
    "\n",
    "dens = gene_density['IGFBP3']/total_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001059533199127473"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
